{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148d6fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T08:48:56.739417Z",
     "iopub.status.busy": "2025-05-25T08:48:56.738703Z",
     "iopub.status.idle": "2025-05-25T08:53:52.610820Z",
     "shell.execute_reply": "2025-05-25T08:53:52.609911Z",
     "shell.execute_reply.started": "2025-05-25T08:48:56.739386Z"
    },
    "papermill": {
     "duration": 0.117665,
     "end_time": "2025-05-24T17:45:30.236568",
     "exception": true,
     "start_time": "2025-05-24T17:45:30.118903",
     "status": "failed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "/tmp/ipykernel_35/343854825.py:71: UserWarning: Argument(s) 'max_holes, max_height, max_width, fill_value' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.5),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train Acc: 78.20% | Val F1: 0.8700\n",
      "Model saved with F1: 0.8700\n",
      "Epoch [2/25] | Train Acc: 91.91% | Val F1: 0.9346\n",
      "Model saved with F1: 0.9346\n",
      "Epoch [3/25] | Train Acc: 96.42% | Val F1: 0.9593\n",
      "Model saved with F1: 0.9593\n",
      "Epoch [4/25] | Train Acc: 98.67% | Val F1: 0.9472\n",
      "Epoch [5/25] | Train Acc: 99.39% | Val F1: 0.9674\n",
      "Model saved with F1: 0.9674\n",
      "Epoch [6/25] | Train Acc: 99.59% | Val F1: 0.9594\n",
      "Epoch [7/25] | Train Acc: 99.59% | Val F1: 0.9675\n",
      "Model saved with F1: 0.9675\n",
      "Epoch [8/25] | Train Acc: 99.80% | Val F1: 0.9675\n",
      "Epoch [9/25] | Train Acc: 99.80% | Val F1: 0.9594\n",
      "Epoch [10/25] | Train Acc: 99.39% | Val F1: 0.9715\n",
      "Model saved with F1: 0.9715\n",
      "Epoch [11/25] | Train Acc: 99.80% | Val F1: 0.9552\n",
      "Epoch [12/25] | Train Acc: 99.59% | Val F1: 0.9796\n",
      "Model saved with F1: 0.9796\n",
      "Epoch [13/25] | Train Acc: 99.69% | Val F1: 0.9716\n",
      "Epoch [14/25] | Train Acc: 99.80% | Val F1: 0.9633\n",
      "Epoch [15/25] | Train Acc: 99.80% | Val F1: 0.9796\n",
      "Epoch [16/25] | Train Acc: 99.90% | Val F1: 0.9796\n",
      "Epoch [17/25] | Train Acc: 99.90% | Val F1: 0.9674\n",
      "Epoch [18/25] | Train Acc: 99.59% | Val F1: 0.9796\n",
      "Epoch [19/25] | Train Acc: 99.90% | Val F1: 0.9756\n",
      "Epoch [20/25] | Train Acc: 99.90% | Val F1: 0.9836\n",
      "Model saved with F1: 0.9836\n",
      "Epoch [21/25] | Train Acc: 99.90% | Val F1: 0.9633\n",
      "Epoch [22/25] | Train Acc: 99.90% | Val F1: 0.9716\n",
      "Epoch [23/25] | Train Acc: 99.90% | Val F1: 0.9716\n",
      "Epoch [24/25] | Train Acc: 99.80% | Val F1: 0.9716\n",
      "Epoch [25/25] | Train Acc: 99.90% | Val F1: 0.9633\n"
     ]
    }
   ],
   "source": [
    "# Soil Classification - PART 1\n",
    "## TRAINING\n",
    "\n",
    "## Imports and Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.serialization import safe_globals\n",
    "import numpy.core.multiarray\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## Dataset Classes\n",
    "class SoilDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.image_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(self.data['soil_type'].unique())\n",
    "        self.class_to_idx = {label: idx for idx, label in enumerate(self.classes)}\n",
    "        self.idx_to_class = {idx: label for label, idx in self.class_to_idx.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row['image_id'])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        label = self.class_to_idx[row['soil_type']]\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        return image, label\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.image_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx]['image_id']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        return image, img_name\n",
    "\n",
    "## Corrected Transforms\n",
    "train_transform = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=256),\n",
    "    A.RandomCrop(height=224, width=224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=256),\n",
    "    A.CenterCrop(height=224, width=224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "##  Data Loading\n",
    "train_dir = '/kaggle/input/soilclassification1/soil_classification-2025/train'\n",
    "test_dir = '/kaggle/input/soilclassification1/soil_classification-2025/test'\n",
    "train_csv = '/kaggle/input/soilclassification1/soil_classification-2025/train_labels.csv'\n",
    "test_csv = '/kaggle/input/soilclassification1/soil_classification-2025/test_ids.csv'\n",
    "\n",
    "full_dataset = SoilDataset(train_csv, train_dir, transform=train_transform)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Apply val transform to validation set\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "## Model Setup\n",
    "model = timm.create_model('tf_efficientnet_b4', pretrained=True, num_classes=len(full_dataset.classes))\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function with label smoothing\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Optimizer with weight decay\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "# Cosine annealing learning rate scheduler\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n",
    "\n",
    "## Training Loop\n",
    "num_epochs = 25\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    train_acc = 100 * correct / total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels_batch in val_loader:\n",
    "            val_images = val_images.to(device)\n",
    "            val_labels_batch = val_labels_batch.to(device)\n",
    "            val_outputs = model(val_images)\n",
    "            _, val_pred = val_outputs.max(1)\n",
    "            val_preds.extend(val_pred.cpu().numpy())\n",
    "            val_labels.extend(val_labels_batch.cpu().numpy())\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Acc: {train_acc:.2f}% | Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'f1': val_f1,\n",
    "            'epoch': epoch\n",
    "        }, 'best_soil_model.pth')\n",
    "        print(f\"Model saved with F1: {val_f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced8b3a-0c0f-44b0-a941-24b75ae66492",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T09:02:13.788315Z",
     "iopub.status.busy": "2025-05-25T09:02:13.787940Z",
     "iopub.status.idle": "2025-05-25T09:02:16.734863Z",
     "shell.execute_reply": "2025-05-25T09:02:16.734059Z",
     "shell.execute_reply.started": "2025-05-25T09:02:13.788292Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final submission saved with 341 rows.\n"
     ]
    }
   ],
   "source": [
    "## INFERENCE\n",
    "#  Prediction with Robust Model Loading\n",
    "test_transform = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=256),\n",
    "    A.CenterCrop(height=224, width=224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# First try secure loading\n",
    "try:\n",
    "    checkpoint = torch.load('best_soil_model.pth', weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "except Exception as e:\n",
    "    print(f\"Secure loading failed: {str(e)}\")\n",
    "    print(\"Attempting safe_globals loading\")\n",
    "    \n",
    "    # Import required modules\n",
    "    from torch.serialization import safe_globals\n",
    "    import numpy as np\n",
    "    from numpy.core.multiarray import scalar\n",
    "    from numpy import dtype\n",
    "    \n",
    "    # Create a custom safe_globals context\n",
    "    def custom_safe_globals():\n",
    "        base_globals = safe_globals()\n",
    "        base_globals.update({\n",
    "            'numpy.core.multiarray.scalar': scalar,\n",
    "            'numpy.dtype': dtype\n",
    "        })\n",
    "        return base_globals\n",
    "    \n",
    "    with torch.serialization._use_custom_globals(custom_safe_globals()):\n",
    "        checkpoint = torch.load('best_soil_model.pth')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Rest of your prediction code...\n",
    "test_dataset = TestDataset(test_csv, test_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "predictions, filenames = [], []\n",
    "with torch.no_grad():\n",
    "    for images, names in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = outputs.max(1)\n",
    "        predictions.extend([full_dataset.idx_to_class[p.item()] for p in preds])\n",
    "        filenames.extend(names)\n",
    "\n",
    "submission = pd.DataFrame({'image_id': filenames, 'soil_type': predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"Final submission saved with {len(submission)} rows.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12375409,
     "sourceId": 102672,
     "sourceType": "competition"
    },
    {
     "datasetId": 7503881,
     "sourceId": 11935538,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 95.247695,
   "end_time": "2025-05-24T17:45:32.818054",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-24T17:43:57.570359",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
