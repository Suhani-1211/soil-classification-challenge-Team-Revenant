{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a148d6fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T08:48:56.739417Z",
     "iopub.status.busy": "2025-05-25T08:48:56.738703Z",
     "iopub.status.idle": "2025-05-25T08:53:52.610820Z",
     "shell.execute_reply": "2025-05-25T08:53:52.609911Z",
     "shell.execute_reply.started": "2025-05-25T08:48:56.739386Z"
    },
    "papermill": {
     "duration": 0.117665,
     "end_time": "2025-05-24T17:45:30.236568",
     "exception": true,
     "start_time": "2025-05-24T17:45:30.118903",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "/tmp/ipykernel_35/343854825.py:71: UserWarning: Argument(s) 'max_holes, max_height, max_width, fill_value' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.5),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train Acc: 78.20% | Val F1: 0.8700\n",
      "Model saved with F1: 0.8700\n",
      "Epoch [2/25] | Train Acc: 91.91% | Val F1: 0.9346\n",
      "Model saved with F1: 0.9346\n",
      "Epoch [3/25] | Train Acc: 96.42% | Val F1: 0.9593\n",
      "Model saved with F1: 0.9593\n",
      "Epoch [4/25] | Train Acc: 98.67% | Val F1: 0.9472\n",
      "Epoch [5/25] | Train Acc: 99.39% | Val F1: 0.9674\n",
      "Model saved with F1: 0.9674\n",
      "Epoch [6/25] | Train Acc: 99.59% | Val F1: 0.9594\n",
      "Epoch [7/25] | Train Acc: 99.59% | Val F1: 0.9675\n",
      "Model saved with F1: 0.9675\n",
      "Epoch [8/25] | Train Acc: 99.80% | Val F1: 0.9675\n",
      "Epoch [9/25] | Train Acc: 99.80% | Val F1: 0.9594\n",
      "Epoch [10/25] | Train Acc: 99.39% | Val F1: 0.9715\n",
      "Model saved with F1: 0.9715\n",
      "Epoch [11/25] | Train Acc: 99.80% | Val F1: 0.9552\n",
      "Epoch [12/25] | Train Acc: 99.59% | Val F1: 0.9796\n",
      "Model saved with F1: 0.9796\n",
      "Epoch [13/25] | Train Acc: 99.69% | Val F1: 0.9716\n",
      "Epoch [14/25] | Train Acc: 99.80% | Val F1: 0.9633\n",
      "Epoch [15/25] | Train Acc: 99.80% | Val F1: 0.9796\n",
      "Epoch [16/25] | Train Acc: 99.90% | Val F1: 0.9796\n",
      "Epoch [17/25] | Train Acc: 99.90% | Val F1: 0.9674\n",
      "Epoch [18/25] | Train Acc: 99.59% | Val F1: 0.9796\n",
      "Epoch [19/25] | Train Acc: 99.90% | Val F1: 0.9756\n",
      "Epoch [20/25] | Train Acc: 99.90% | Val F1: 0.9836\n",
      "Model saved with F1: 0.9836\n",
      "Epoch [21/25] | Train Acc: 99.90% | Val F1: 0.9633\n",
      "Epoch [22/25] | Train Acc: 99.90% | Val F1: 0.9716\n",
      "Epoch [23/25] | Train Acc: 99.90% | Val F1: 0.9716\n",
      "Epoch [24/25] | Train Acc: 99.80% | Val F1: 0.9716\n",
      "Epoch [25/25] | Train Acc: 99.90% | Val F1: 0.9633\n"
     ]
    }
   ],
   "source": [
    "# Soil Classification - Final Corrected Notebook\n",
    "\n",
    "## 1. Imports and Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.serialization import safe_globals\n",
    "import numpy.core.multiarray\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## 2. Dataset Classes\n",
    "class SoilDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.image_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(self.data['soil_type'].unique())\n",
    "        self.class_to_idx = {label: idx for idx, label in enumerate(self.classes)}\n",
    "        self.idx_to_class = {idx: label for label, idx in self.class_to_idx.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row['image_id'])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        label = self.class_to_idx[row['soil_type']]\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        return image, label\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.image_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx]['image_id']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        return image, img_name\n",
    "\n",
    "## 3. Corrected Transforms\n",
    "train_transform = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=256),\n",
    "    A.RandomCrop(height=224, width=224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=256),\n",
    "    A.CenterCrop(height=224, width=224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "## 4. Data Loading\n",
    "train_dir = '/kaggle/input/soilclassification1/soil_classification-2025/train'\n",
    "test_dir = '/kaggle/input/soilclassification1/soil_classification-2025/test'\n",
    "train_csv = '/kaggle/input/soilclassification1/soil_classification-2025/train_labels.csv'\n",
    "test_csv = '/kaggle/input/soilclassification1/soil_classification-2025/test_ids.csv'\n",
    "\n",
    "full_dataset = SoilDataset(train_csv, train_dir, transform=train_transform)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Apply val transform to validation set\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "## 5. Model Setup\n",
    "model = timm.create_model('tf_efficientnet_b4', pretrained=True, num_classes=len(full_dataset.classes))\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function with label smoothing\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Optimizer with weight decay\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "# Cosine annealing learning rate scheduler\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n",
    "\n",
    "## 6. Training Loop\n",
    "num_epochs = 25\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    train_acc = 100 * correct / total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels_batch in val_loader:\n",
    "            val_images = val_images.to(device)\n",
    "            val_labels_batch = val_labels_batch.to(device)\n",
    "            val_outputs = model(val_images)\n",
    "            _, val_pred = val_outputs.max(1)\n",
    "            val_preds.extend(val_pred.cpu().numpy())\n",
    "            val_labels.extend(val_labels_batch.cpu().numpy())\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Acc: {train_acc:.2f}% | Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'f1': val_f1,\n",
    "            'epoch': epoch\n",
    "        }, 'best_soil_model.pth')\n",
    "        print(f\"Model saved with F1: {val_f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97b644",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T09:30:46.889763Z",
     "iopub.status.busy": "2025-05-25T09:30:46.889448Z",
     "iopub.status.idle": "2025-05-25T09:30:46.920124Z",
     "shell.execute_reply": "2025-05-25T09:30:46.919274Z",
     "shell.execute_reply.started": "2025-05-25T09:30:46.889737Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db151c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T09:30:50.824525Z",
     "iopub.status.busy": "2025-05-25T09:30:50.823747Z",
     "iopub.status.idle": "2025-05-25T09:30:50.828955Z",
     "shell.execute_reply": "2025-05-25T09:30:50.827947Z",
     "shell.execute_reply.started": "2025-05-25T09:30:50.8245Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example paths (adjust if needed based on unzip location)\n",
    "data_dir = \"/kaggle/input/soil-classification-part-2/soil_competition-2025\"\n",
    "train_csv = os.path.join(data_dir, \"train_labels.csv\")\n",
    "train_img_dir = os.path.join(data_dir, \"train\")\n",
    "test_img_dir = os.path.join(data_dir, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c535601",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T09:30:54.972448Z",
     "iopub.status.busy": "2025-05-25T09:30:54.972125Z",
     "iopub.status.idle": "2025-05-25T09:30:54.9794Z",
     "shell.execute_reply": "2025-05-25T09:30:54.978539Z",
     "shell.execute_reply.started": "2025-05-25T09:30:54.972424Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be013802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T09:31:02.165281Z",
     "iopub.status.busy": "2025-05-25T09:31:02.164974Z",
     "iopub.status.idle": "2025-05-25T09:31:02.176615Z",
     "shell.execute_reply": "2025-05-25T09:31:02.175834Z",
     "shell.execute_reply.started": "2025-05-25T09:31:02.165258Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df_balanced, test_size=0.2, stratify=df_balanced['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bcb435",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T09:31:55.505042Z",
     "iopub.status.busy": "2025-05-25T09:31:55.504716Z",
     "iopub.status.idle": "2025-05-25T09:31:55.510819Z",
     "shell.execute_reply": "2025-05-25T09:31:55.510127Z",
     "shell.execute_reply.started": "2025-05-25T09:31:55.50502Z"
    }
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d3a341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T09:32:10.002409Z",
     "iopub.status.busy": "2025-05-25T09:32:10.001967Z",
     "iopub.status.idle": "2025-05-25T09:32:10.012291Z",
     "shell.execute_reply": "2025-05-25T09:32:10.011401Z",
     "shell.execute_reply.started": "2025-05-25T09:32:10.002374Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(SoilDataset(train_df, train_img_dir, transform=transform_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(SoilDataset(val_df, train_img_dir, transform=transform_test), batch_size=batch_size)\n",
    "test_loader = DataLoader(SoilDataset(pd.DataFrame({'image_id': os.listdir(test_img_dir)}), test_img_dir, transform=transform_test, test=True), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06771de8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T09:39:45.881719Z",
     "iopub.status.busy": "2025-05-25T09:39:45.880772Z",
     "iopub.status.idle": "2025-05-25T09:39:45.8875Z",
     "shell.execute_reply": "2025-05-25T09:39:45.886701Z",
     "shell.execute_reply.started": "2025-05-25T09:39:45.88169Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        evaluate_model(model)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e813a510",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T09:40:16.115243Z",
     "iopub.status.busy": "2025-05-25T09:40:16.114941Z",
     "iopub.status.idle": "2025-05-25T10:12:09.774403Z",
     "shell.execute_reply": "2025-05-25T10:12:09.7733Z",
     "shell.execute_reply.started": "2025-05-25T09:40:16.115221Z"
    }
   },
   "outputs": [],
   "source": [
    "train_model(model, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12375409,
     "sourceId": 102672,
     "sourceType": "competition"
    },
    {
     "datasetId": 7503881,
     "sourceId": 11935538,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 95.247695,
   "end_time": "2025-05-24T17:45:32.818054",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-24T17:43:57.570359",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
